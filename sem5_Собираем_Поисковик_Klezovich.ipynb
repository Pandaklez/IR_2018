{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5    \n",
    "## Собираем поисковик \n",
    "\n",
    "![](https://bilimfili.com/wp-content/uploads/2017/06/bir-urune-emek-vermek-o-urune-olan-deger-algimizi-degistirir-mi-bilimfilicom.jpg) \n",
    "\n",
    "\n",
    "Мы уже все знаем, для того чтобы сделать поисковик. Осталось соединить все части вместе.    \n",
    "Итак, для поисковика нам понадобятся:         \n",
    "**1. База документов **\n",
    "> в первом дз - корпус Друзей    \n",
    "в сегодняшнем дз - корпус юридических вопросов-ответов    \n",
    "в итоговом проекте - корпус Авито   \n",
    "\n",
    "**2. Функция индексации**                 \n",
    "Что делает: собирает информацию о корпусе, по которуму будет происходить поиск      \n",
    "Своя для каждого поискового метода:       \n",
    "> A. для обратного индекса она создает обратный индекс (чудо) и сохраняет статистики корпуса, необходимые для Okapi BM25 (средняя длина документа в коллекции, количество доков ... )             \n",
    "> B. для поиска через word2vec эта функция создает вектор для каждого документа в коллекции путем, например, усреднения всех векторов коллекции       \n",
    "> C. для поиска через doc2vec эта функция создает вектор для каждого документа               \n",
    "\n",
    "   Не забывайте сохранить все, что насчитает эта функция. Если это будет происходить налету во время поиска, понятно, что он будет работать сто лет     \n",
    "   \n",
    "**3. Функция поиска**     \n",
    "Можно разделить на две части:\n",
    "1. функция вычисления близости между запросом и документом    \n",
    "> 1. для индекса это Okapi BM25\n",
    "> 2. для w2v и d2v это обычная косинусная близость между векторами          \n",
    "2. ранжирование (или просто сортировка)\n",
    "\n",
    "\n",
    "Время все это реализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индексация\n",
    "## Word2Vec\n",
    "### Задание 1\n",
    "Загрузите любую понравившуюся вам word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', module='gensim')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если модель без тэгов\n",
    "model_path = 'araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model'\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если модель с POS-тэггингом\n",
    "# model = KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "Напишите функцию индексации для поиска через word2vec. Она должна для каждого документа из корпуса строить вектор.   \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только  вектор, но и опознователь текста, которому он принадлежит. \n",
    "Для поисковика это может быть url страницы, для поиска по текстовому корпусу сам текст.\n",
    "\n",
    "> В качестве документа для word2vec берите **параграфы** исходного текста, а не весь текст целиком. Так вектора будут более осмысленными. В противном случае можно получить один очень общий вектор, релевантый совершенно разным запросам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"articles_sudbiblioteka\"\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_spaces(text):\n",
    "    \n",
    "    processed_text = text.replace('\\n', ' ').replace('\\n\\n', ' ').replace('ул. ', 'ул.').replace('г. ', 'г.').replace('гор. ', 'гор.').replace('с. ', 'с.')\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# убираем пробел после инициалов перед фамилией\n",
    "def clear_abbrs(processed_text):\n",
    "    initials = re.compile(r'[А-Я]{1}\\.[А-Я]{1}\\. [А-Я][а-яё]+')\n",
    "    counter = len(initials.findall(processed_text))\n",
    "\n",
    "    for s in range(counter):\n",
    "        get_abbrs = initials.search(processed_text)\n",
    "        i = get_abbrs.span()[0] + 4\n",
    "        processed_text = processed_text[:i] + processed_text[i+1:]\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# делим текст на предложения при помощи регулярного выражения\n",
    "def split_text(processed_text):\n",
    "    \n",
    "    text_splitted = re.split(r'(\\. +[А-Я]{1} *[а-яё]+)', processed_text)\n",
    "    last_word = re.compile(r'[А-Я]{1} *[а-яё]+')\n",
    "    normal_sentences = [text_splitted[0] + '.']\n",
    "\n",
    "    for i in range(1, len(text_splitted), 2):\n",
    "        if i + 1 <= len(text_splitted)-1:\n",
    "            beginning = last_word.findall(text_splitted[i])[0]\n",
    "            normal_sentences.append(beginning + text_splitted[i+1] + '.')\n",
    "        elif i == len(text_splitted)-1:\n",
    "            beginning = last_word.findall(text_splitted[i])[0]\n",
    "            normal_sentences.append(beginning)\n",
    "    return normal_sentences\n",
    "\n",
    "\n",
    "def get_sentences(text):\n",
    "    text = no_spaces(text)\n",
    "    text = clear_abbrs(text)\n",
    "    sentences = split_text(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# делим текст на куски по n предложений\n",
    "# (функция принимает на вход список из предложений-строк, полученный на предыдущем шаге)\n",
    "def split_paragraph(list_of_sentences, n):\n",
    "\n",
    "    l = len(list_of_sentences)\n",
    "\n",
    "    n_chunks = []\n",
    "    chunk = ''\n",
    "\n",
    "    for i in range(0, l, n):\n",
    "        for j in range(n):\n",
    "            if i+j < l:\n",
    "                chunk += list_of_sentences[i+j] + ' '\n",
    "            else:\n",
    "                continue\n",
    "        n_chunks.append(chunk)\n",
    "        chunk = ''\n",
    "    return n_chunks\n",
    "\n",
    "# main function here\n",
    "def splitter(text, n):\n",
    "    \"\"\"\n",
    "    :return: split_sentences as a list of strings\n",
    "    \"\"\"\n",
    "    normal_sentences = get_sentences(text)\n",
    "    split_sentences = split_paragraph(normal_sentences, n)\n",
    "    return split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function works with preprocessing func result\n",
    "def get_w2v_vectors(lemmas_arr, model):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    if len(lemmas_arr) == 0:\n",
    "        doc_vec = None\n",
    "    else:\n",
    "        vectors = []\n",
    "        for element in lemmas_arr:\n",
    "            try:\n",
    "                vec = model.wv[element]\n",
    "                # len(vec) this gives us 300\n",
    "            except KeyError:\n",
    "                continue\n",
    "            vectors.append(vec)\n",
    "        vec_sum = np.zeros(300)\n",
    "        for v in vectors:\n",
    "            vec_sum += v\n",
    "        doc_vec = vec_sum/len(vectors)   # усредненный вектор как опознаватель\n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "with open(\"articles_sudbiblioteka\\\\article\\\\3.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    for sentence in splitter(text, 1):\n",
    "        lemmas_arr = preprocessing(sentence)\n",
    "        #print(lemmas_arr)\n",
    "        vec = get_w2v_vectors(lemmas_arr, model)\n",
    "        print(type(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main_dir = 'articles_sudbiblioteka\\\\article'\n",
    "file_names = os.listdir(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_w2v_base(file_names, model):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    dc = []\n",
    "    for name in tqdm(file_names[:1000]):  # testing on a fraction of the data to save time\n",
    "        with open(\"articles_sudbiblioteka\\\\article\\\\\" + name, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            for sentence in splitter(text, 1):\n",
    "                sentence_vec = get_w2v_vectors(preprocessing(sentence), model)\n",
    "                dc.append([sentence_vec, sentence, text])\n",
    "    #with open('saved_w2v_base.json', 'w') as outfile:\n",
    "    #    json.dump(dc, outfile, ensure_ascii=False)\n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755bdc8f739541f8af75280b291f3090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dc = save_w2v_base(file_names, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dc, columns=['sentence_vec','sentence', 'text']).to_csv(\"w2v_dc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pd.read_csv(\"w2v_dc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pd.DataFrame(dc, columns=['sentence_vec','sentence', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "### Задание 3\n",
    "Напишите функцию обучения doc2vec на юридических текстах, и получите свою кастомную d2v модель. \n",
    "> Совет: есть мнение, что для обучения doc2vec модели не нужно удалять стоп-слова из корпуса. Они являются важными семантическими элементами.      \n",
    "\n",
    "Важно! В качестве документа для doc2vec берите **параграфы** исходного текста, а не весь текст целиком. И не забывайте про предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(data):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, min_alpha=0.025, epochs=100, workers=4, dm=1)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_tmpfile(\"d2v_jura\")\n",
    "model_d2v = train_doc2vec(dc['sentence'])\n",
    "model_d2v.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = Doc2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Напишите функцию индексации для поиска через doc2vec. Она должна для каждого документа из корпуса получать вектор.    \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только вектор, но и опознователь текста, которому он принадлежит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d2v_vectors(lemmas_arr, model_d2v):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    # model.infer_vector([\"закон\", \"договор\"])\n",
    "    model_d2v.random.seed(100)  # ensure same results\n",
    "    if len(lemmas_arr) == None:\n",
    "        doc_vec = None\n",
    "    else:\n",
    "        doc_vec = model_d2v.infer_vector(lemmas_arr)\n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "with open(\"articles_sudbiblioteka\\\\article\\\\3.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    for sentence in splitter(text, 1):\n",
    "        lemmas_arr = preprocessing(sentence)\n",
    "        #print(lemmas_arr)\n",
    "        doc_vec = get_d2v_vectors(lemmas_arr, model_d2v)\n",
    "        #print(doc_vec)\n",
    "    #print(len(doc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_d2v_base(file_names, model_d2v):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    d = []\n",
    "    for name in tqdm(file_names[:3000]):\n",
    "        with open(\"articles_sudbiblioteka\\\\article\\\\\" + name, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            for sentence in splitter(text, 1):\n",
    "                sentence_vec = get_d2v_vectors(preprocessing(sentence, del_stopwords=False), model_d2v)\n",
    "                d.append([sentence_vec, sentence, text])\n",
    "    # with open('saved_d2v_base.json', 'w') as outfile:\n",
    "    #     json.dump(d, outfile, ensure_ascii=False)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec129c3b762434e8c26951e197358ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d = save_d2v_base(file_names, model_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(d, columns=['sentence_vec','sentence', 'text']).to_csv(\"d2v_d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(d, columns=['sentence_vec','sentence', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Судья Высшего Арбитражного Суда Российской Ф...\n",
       "1        В соответствии с частью 1 статьи 292 Арбитражн...\n",
       "2        Подающие данное заявление граждане Хорошавин Ю...\n",
       "3        Оспариваемые судебные акты приняты в отношении...\n",
       "4        Возвращение к проверке в порядке надзора судеб...\n",
       "5        Данное заявление не отвечает этим требованиям,...\n",
       "6        Руководствуясь пунктами 1, 3 части 1 статьи 29...\n",
       "7                                 Судья Н.А.КСЕНОФОНТОВА. \n",
       "8          Судья Высшего Арбитражного Суда Российской Ф...\n",
       "9        На основании изложенного и руководствуясь стат...\n",
       "10                                 Судья В.Н.АЛЕКСАНДРОВ. \n",
       "11         Высший Арбитражный Суд Российской Федерации ...\n",
       "12       Заявление принято к производству определением ...\n",
       "13       Для решения вопроса о наличии оснований для пе...\n",
       "14       Руководствуясь абзацем 2 части 3 статьи 299 Ар...\n",
       "15       Председательствующий судья Е.Н.ЗАРУБИНА Судья ...\n",
       "16         Судья Высшего Арбитражного Суда Российской Ф...\n",
       "17       Руководствуясь статьей 295 Арбитражного процес...\n",
       "18       Направить копию настоящего определения заявите...\n",
       "19                                     Судья А.М.ХАЧИКЯН. \n",
       "20       ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ КАССАЦИОННО...\n",
       "21       По ст. 325 ч. 2 УК РФ К. оправдан за отсутстви...\n",
       "22       Постановлено взыскать с К. в пользу Н.Н. в воз...\n",
       "23       Заслушав доклад судьи Бурова А.А., объяснения ...\n",
       "24       Преступления совершены 16 июля 1995 года  в  г...\n",
       "25       Волгограде  при указанных в приговоре обстояте...\n",
       "26       В судебном заседании К. виновным себя не призн...\n",
       "27       В кассационной жалобе (основной и дополнительн...\n",
       "28                          Эти преступления  совершил М. \n",
       "29       Его же М. оговорил , чтобы облегчить свою учас...\n",
       "                               ...                        \n",
       "35493    Вместе с тем,  суд  правильно установив обстоя...\n",
       "35494    В материалах дела отсутствуют доказательства н...\n",
       "35495    Исходя из изложенного и руководствуясь п. 5 ч....\n",
       "35496    С частичным присоединением  неотбытого  наказа...\n",
       "35497    В остальном судебные решения оставить без изме...\n",
       "35498      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35499    На основании изложенного и руководствуясь стат...\n",
       "35500                                 Судья А.И.ЧИСТЯКОВ. \n",
       "35501      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35502    На основании изложенного и руководствуясь стат...\n",
       "35503                                 Судья А.И.ЧИСТЯКОВ. \n",
       "35504      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35505    На основании изложенного и руководствуясь стат...\n",
       "35506                                    Судья М.Ф.ЮХНЕЙ. \n",
       "35507      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35508    Руководствуясь статьей 295 Арбитражного процес...\n",
       "35509                              Судья И.М.МАРАМЫШКИНА. \n",
       "35510      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35511    Руководствуясь статьей 295 Арбитражного процес...\n",
       "35512                                  Судья А.Е.БЕРЕЗИЙ. \n",
       "35513      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35514    Руководствуясь статьей 295 Арбитражного процес...\n",
       "35515                              Судья И.М.МАРАМЫШКИНА. \n",
       "35516      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35517    Руководствуясь статьей 295 Арбитражного процес...\n",
       "35518    Направить настоящее определение ЗАО \" Краснода...\n",
       "35519                                Судья ЛОКТЕНКО Н.И.. \n",
       "35520      Судья Высшего Арбитражного Суда Российской Ф...\n",
       "35521    Исходя из изложенного и руководствуясь статьям...\n",
       "35522                                   Судья И.В.ПАНОВА. \n",
       "Name: sentence, Length: 35523, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv(\"d2v_d.csv\")\n",
    "d['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса функцией поиска является Okapi BM25. Она у вас уже должна быть реализована."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "def inverted_index(texts) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    i = 0\n",
    "    for text in texts:\n",
    "        for word in text.split(' '):\n",
    "            if i not in index[word]:\n",
    "                index[word].append(i)\n",
    "        i += 1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1=2.0, b=0.75, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    score = log((N - n + 0.5)/(n + 0.5)) * (k1 + 1) * qf / (qf + k1 * (1 - b + b * (dl / avgdl)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(query, documents) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "        \n",
    "    return score\n",
    "\n",
    "\n",
    "def get_search_result() -> list:\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция измерения близости между векторами нам пригодится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "import numpy as np \n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите функцию для поиска через word2vec и для поиска через doc2vec, которая по входящему запросу выдает отсортированную выдачу документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_w2v(query, model_vectors, doc_text, model):\n",
    "    lemmas_query = preprocessing(query, del_stopwords=False)\n",
    "    query_vector = get_w2v_vectors(lemmas_query, model)\n",
    "    # print(type(query_vector))\n",
    "    result = []\n",
    "    i = 0\n",
    "    for vec in model_vectors:\n",
    "        # print(type(vec))\n",
    "        similar = similarity(query_vector, vec)\n",
    "        result.append((doc_text[i], similar))\n",
    "        i += 1\n",
    "    res = pd.DataFrame(result, columns=['doc_text','similarity'])\n",
    "    return res.sort_values('similarity', ascending=False)  # sort by similarity\n",
    "\n",
    "def search_d2v(query, model_vectors, doc_text, model_d2v):\n",
    "    lemmas_query = preprocessing(query, del_stopwords=False)\n",
    "    query_vector = get_d2v_vectors(lemmas_query, model_d2v)\n",
    "    result = []\n",
    "    i = 0\n",
    "    for vec in model_vectors:\n",
    "        similar = similarity(query_vector, vec)\n",
    "        result.append((doc_text[i], similar))\n",
    "        i += 1\n",
    "    res = pd.DataFrame(result, columns=['doc_text','similarity'])\n",
    "    return res.sort_values('similarity', ascending=False)  # sort by similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               doc_text  similarity\n",
      "26    ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.845766\n",
      "5116  ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.821300\n",
      "7574  ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.809519\n",
      "9383  ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.804492\n",
      "5192  ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nОПРЕДЕЛЕНИ...    0.804492\n"
     ]
    }
   ],
   "source": [
    "w2v_search = search_w2v('В судебном заседании К. виновным себя не признал.',\n",
    "                        dc['sentence_vec'], dc['text'], model)\n",
    "print(w2v_search[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                doc_text  similarity\n",
      "26     ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    1.000000\n",
      "31680  ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.915588\n",
      "7574   ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.891737\n",
      "33123  ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.860533\n",
      "26385  ВЕРХОВНЫЙ СУД РОССИЙСКОЙ ФЕДЕРАЦИИ\\nКАССАЦИОНН...    0.845447\n"
     ]
    }
   ],
   "source": [
    "d2v_search = search_d2v('В судебном заседании К. виновным себя не признал.',\n",
    "                        d['sentence_vec'], d['text'], model_d2v)\n",
    "print(d2v_search[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, model_vectors, doc_text, model, search_method):\n",
    "    if search_method == 'inverted_index':\n",
    "        search_result = search_inv_index()\n",
    "    elif search_method == 'word2vec':\n",
    "        search_result = search_w2v()\n",
    "    elif search_method == 'doc2vec':\n",
    "        search_result = search_d2v()\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
